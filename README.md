# Hi there, I'm Aditya ğŸ‘‹

**Software Engineer (ML / AI Systems)** | **MS in Computer Science @ Georgia Tech** | **LLM & ML Infrastructure Enthusiast**

Iâ€™m currently pursuing my **MS in Computer Science at Georgia Tech** (GPA: 3.75/4.0, graduating May 2026), with a strong focus on **Machine Learning systems, LLM inference, and scalable data pipelines**.  
I bring **5+ years of industry experience** working on production Software Engineering and ML products, large-scale data systems, and performance-critical services at **Google X, Google Research**, and startups.

---

## ğŸš€ About Me

- ğŸ“ MS CS @ **Georgia Institute of Technology** (ML, DL, NLP, Systems for ML, Conversational AI)
- ğŸ§  ML Engineer with hands-on experience across **classical ML, deep learning, GenAI, and MLOps**
- âš™ï¸ Strong interest in **LLM inference systems, tail-latency mitigation, retrieval systems, and ML infrastructure**
- ğŸŒ Built and scaled ML systems used globally across **Search, climate risk modeling, and language learning**
- ğŸ‘¥ Enjoy mentoring, code quality improvements, and collaborating with domain experts (scientists & researchers)

---

## ğŸ’¼ Professional Experience

### **Infocusp Innovations** â€” Software Engineer 2  
**Client: Google X (Moonshot Factory)** | *Jan 2022 â€“ Jun 2025*

- Built **multi-tenant hazard risk forecasting models** (1â€“5 year horizons) for heatwaves and wildfires using geospatial data
- Achieved **PR-AUC 0.96** (seismic classification) and **Dice 0.73** (S-wave segmentation) on DAS sensor data
- Reduced experiment analysis runtime from **4 hours â†’ 20 minutes**
- Scaled ingestion pipelines using **Google Earth Engine, Pub/Sub, and GCP buckets**
- Reduced class-skew-induced model drift from **24% â†’ 1%**
- Added explainability with **SHAP, Grad-CAM, reliability curves**
- Mentored junior engineers; increased test coverage from **56% â†’ 100%**

---

### **Infocusp Innovations** â€” Software Engineer  
**Client: Google Research** | *May 2020 â€“ Dec 2021*

- Owned product features and data pipelines for a **language learning platform** with **150K+ daily impressions**
- Platform featured on **Google Search results and Search Labs**
- Designed notification cron jobs; reduced runtime from **5 hours â†’ 40 minutes**
- Built **gRPC microservices in C++**
- Diagnosed and fixed p99 latency regressions, achieving **~30% latency reduction**

---
## ğŸ§© Work Portfolio

A selection of real-world products and platforms Iâ€™ve contributed to through industry roles and client engagements:

- **Google X â€“ The Moonshot Factory**  
  Climate risk forecasting and geospatial ML systems  
  ğŸŒ https://x.company/

- **Google Research**  
  Language learning platform featured on Google Search & Search Labs  
  ğŸŒ https://research.google/

- **Bryte Labs**  
  Biometric data ingestion, modeling, and analytics platform  
  ğŸŒ https://www.bryte.com/

- **Infocusp Innovations**  
  Engineering partner delivering ML and data systems for global clients  
  ğŸŒ https://www.infocusp.com/

---

## ğŸ§  Academic & Research Projects

### ğŸ”¥ Risk-Triggered Mid-Flight Request Migration (LLM Inference)
- Built a **risk-triggered migration mechanism** for prefillâ€“decode disaggregation in **vLLM**
- Mitigated decode stragglers under heavy-tailed workloads
- Achieved **65% TBT reduction** and **~25% lower end-to-end latency**
- Modeled KV-cache growth â†’ time-between-tokens using telemetry + online calibration
- Implemented **stability-guarded migrations** (e.g., BF16 â†’ FP8) on H100 GPUs

---

### ğŸ” Hybrid Vector & Metadata Retrieval Search
- Designed a hybrid retrieval engine over **100K documents (768-D embeddings)**
- Implemented a **metadata-aware IVF index** for selective filtering
- Achieved **77Ã— speedup** for high-selectivity queries (<1%)
- Benchmarked IVF-Flat, IVF-PQ, bitmap filtering, and brute-force baselines
- Identified ANN cross-over points across selectivity regimes

---

### ğŸ“„ Explainable RAG for SEC 10-K QA
- Built a **layout-aware, citation-grounded RAG pipeline**
- Implemented deterministic **faithfulness & verification checks**
- Designed retrieval and generation evaluation framework on **FinDER dataset**

---

### ğŸ§© Parameter-Efficient LLM Fine-Tuning
- Benchmarked **Full FT vs LoRA vs QLoRA** on a 7B LLM (50K NLI samples)
- LoRA within **~1%**, QLoRA within **~2%** of full fine-tuning (Acc / Macro-F1)
- Reduced peak GPU memory by **65â€“70%**
- Applied **context distillation** using KL-regularized ICL soft targets

---

### ğŸ§  Transformer Language Model from Scratch

- Implemented a **GPT-style Transformer language model** end-to-end in PyTorch, including embeddings, pre-norm Transformer blocks, RMSNorm, SwiGLU feed-forward layers, and causal multi-head self-attention with RoPE.
- Built the **full training pipeline** from scratch: memory-mapped data loading, batching, AdamW optimization, gradient clipping, checkpointing, and cosine learning rate scheduling.
- Trained a ~**17M parameter model** on the **TinyStories** dataset, achieving **â‰¤ 1.8 validation loss**, with further improvements at higher token budgets.
- Implemented **autoregressive decoding** with temperature scaling and **top-p (nucleus) sampling** for text generation.
- (Optional) Implemented a **byte-level BPE tokenizer** with custom merge rules, encoding, and decoding.
- Passed a comprehensive **pytest-based test suite** covering attention, normalization, transformer blocks, training, and inference.

**Tech:** PyTorch, Transformers, RoPE, RMSNorm, SwiGLU, AdamW, CUDA, Pytest

> **Note:** Most academic project repositories are private due to Georgia Techâ€™s Office of Student Integrity policies.  
> Iâ€™m happy to discuss designs, trade-offs, and resultsâ€”or provide demos for recruiting purposes.

---

## ğŸ› ï¸ Tech Stack

**Languages:** Python â€¢ C++ â€¢ JavaScript â€¢ TypeScript â€¢ SQL  
**ML / AI:** PyTorch â€¢ TensorFlow â€¢ HuggingFace â€¢ vLLM â€¢ RAG â€¢ Vector Search â€¢ LoRA / QLoRA  
**MLOps & Observability:** MLflow â€¢ Weights & Biases â€¢ Prometheus â€¢ Grafana  
**Data & Infra:** GCP (GEE, Pub/Sub, BigQuery, Dataflow, Cloud Run) â€¢ AWS â€¢ Terraform  
**Systems:** Docker â€¢ Kubernetes â€¢ gRPC â€¢ CI/CD â€¢ Distributed Systems  

---

## ğŸ“š Currently Exploring

- LLM inference optimization & scheduling
- Retrieval evaluation and faithfulness in RAG
- Systems for Machine Learning
- Tail-latency mitigation techniques

---

## ğŸ“« Letâ€™s Connect

- ğŸ’¼ LinkedIn: https://linkedin.com/in/adityapandit99  
- ğŸ§‘â€ğŸ’» GitHub: https://github.com/adityap99  
- ğŸ“§ Email: adityaspandit99@gmail.com  

---

ğŸ’¡ **Open to roles in:**  
ML Engineering â€¢ AI Systems â€¢ LLM Infrastructure â€¢ Software Engineering (ML-heavy)

ğŸ“ **Education:**  
MS Computer Science â€” Georgia Tech  
BTech ICT â€” DA-IICT
